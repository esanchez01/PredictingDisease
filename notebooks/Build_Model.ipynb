{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src') \n",
    "\n",
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Libraries for machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Libraries for plotting curves\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from itertools import cycle\n",
    "\n",
    "# Importing script\n",
    "import etl as etl\n",
    "import visualize_data as visualize_data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disease Risk Classification Model\n",
    "\n",
    "The purpose of this notebook is to explore a variety of models and tune them in order to figure out which is best for disease risk classification. According to the article \"Comparing different supervised machine learning algorithms for disease prediction\" by Shahadat Uddin et al. (https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-019-1004-8), Support Vector Machines, Naive Bayes, and Random Forest are some of the most common machine learning algorithms applied to disease prediction. We will explore these along with other algorithms such as Logistic Regression and K Nearest Neighbors in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get straight into it and simulate a data set of 5,000 individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_gwas_fp = '../testdata/gwas/gwas_simulate.tsv' \n",
    "maf_fp = '../references/snp_mafs.txt'\n",
    "simulated_data = etl.simulate_data(simulated_gwas_fp, maf_fp, 5000)\n",
    "simulated_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not be using the simulated data above for building the model. Given that the class label (disease risk category) was assigned to an individual depending on the weighted sum of all and only the SNPs in this data set, it would be too easy for a machine learning model to figure this out. Essentially, the above data set is a simulated 'ground truth'. In machine learning problems you typically don't work with all the variables that determine the label so something needs to be done about this.\n",
    "\n",
    "As a solution, we will be making classifications based on a subset of SNPs in the above data set. To do this, we will be using a separate GWAS to inform us of SNPs that are most important in predicting disease. Only those SNPs that are in the above data set and the new GWAS will be used. The model will then be trained on this subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in the other GWAS data and filter the above data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gwas_fp = '../testdata/gwas/gwas_model.tsv' \n",
    "model_data = pd.read_csv(model_gwas_fp, sep='\\t')\n",
    "subset = set(simulated_data.columns).intersection(model_data['SNPS'].unique())\n",
    "new_columns = list(subset)+['Class']\n",
    "\n",
    "data = simulated_data[new_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a training and test set on the above data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get proportion of each class\n",
    "prop_per_class = y.value_counts(normalize=True)\n",
    "prop_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train different models on these sets and assess their results.\n",
    "\n",
    "**Note:**\n",
    "\n",
    ">It is important to note that accuracy is not the only metric that is important here to assess the performance of the models. Since we are working with predicting the disease risk of individuals, **Type I** and **Type II** errors are also very important. It is potentially dangerous to classify an individual as low risk when they are actually high risk (Type II, False Negative). Additionally, classifying an individual as high risk when they are actually low risk could cause the individual some unecessary stress within themselves and their families (Type I, False Positive). Therefore, it is important to control for such errors in our model. To do so, we will prioritize the maximization of **Recall** (TP/TP+FN) since it is an indicator of the dangerous False Negatives in our model but at the same time attempt to maximize **Precision** (TP/TP+FP) since it is an indicator of False Positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression()\n",
    "lg.fit(X_train, y_train)\n",
    "\n",
    "accuracy_lg = lg.score(X_test, y_test)*100\n",
    "print('The accuracy for the Logistic Regression model is {}%'.format(accuracy_lg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's attempt to improve the model using grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_parameters = {'tol':[.1, .001, .0001], 'C':[10, 1, .1]}\n",
    "lg = LogisticRegression()\n",
    "clf1 = GridSearchCV(lg, lg_parameters)\n",
    "\n",
    "clf1.fit(X_train, y_train)\n",
    "preds_lg = clf1.predict(X_test)\n",
    "accuracy_lr = np.mean(y_test == preds_lg)*100\n",
    "\n",
    "\n",
    "print('The accuracy for the refined Logistic Regression model is {}%'.format(accuracy_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(clf1.best_params_)\n",
    "# print(clf1.best_score_)\n",
    "# display(pd.DataFrame.from_dict(clf1.cv_results_).sort_values('rank_test_score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Low Risk', 'Medium Risk', 'High Risk']\n",
    "print(classification_report(y_test, preds_lg, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv = LogisticRegression()\n",
    "\n",
    "cv_scores_LR = cross_val_score(lr_cv, X, y, cv=5)\n",
    "\n",
    "print('Cross-Validation Scores: ' + str(cv_scores_LR))\n",
    "print('Mean of Cross-Validation Scores: {}%'.format(np.mean(cv_scores_LR)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "params = {'tol':[.1, .0001, 1e-5], 'C':[10, 1, .1]}\n",
    "\n",
    "lr_gscv = GridSearchCV(lr, params, cv=5)\n",
    "\n",
    "lr_gscv.fit(X_train, y_train)\n",
    "preds_lr = lr_gscv.predict(X_test)\n",
    "accuracy_lr_gscv = np.mean(y_test == preds_lr)*100\n",
    "\n",
    "print('The accuracy for the refined Logistic Regression model is {}%'.format(accuracy_lr_gscv))\n",
    "\n",
    "# NOTE: the following commented-out code is to print\n",
    "# what the best parameter values are and \n",
    "# the mean cross-validated score of the best estimator\n",
    "print('The best parameters are:')\n",
    "for key, val in lr_gscv.best_params_.items():\n",
    "    print(str(key) + ':', val)\n",
    "# print('\\nThe best score for the refined Logistic Regression model is {}%'.format(lr_gscv.best_score_*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting ROC and P-R Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "lr_best = LogisticRegression(C=10, tol=0.0001)\n",
    "lr_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass P-R curve\n",
    "visualize_data.plot_precision_recall(\n",
    "    'Logistic Regression', lr_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "lr_best = LogisticRegression(C=10, tol=0.0001)\n",
    "lr_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass ROC curve\n",
    "visualize_data.plot_multiclass_roc(\n",
    "    'Logistic Regression', lr_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "accuracy_knn = knn.score(X_test, y_test)*100\n",
    "print('The accuracy for the K Nearest Neighbors model is {}%'.format(accuracy_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's attempt to improve the model using grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_parameters = {'n_neighbors':[10, 5, 3, 1], 'p':[2, 1]}\n",
    "knn = KNeighborsClassifier()\n",
    "clf2 = GridSearchCV(knn, knn_parameters)\n",
    "\n",
    "clf2.fit(X_train, y_train)\n",
    "preds_knn = clf2.predict(X_test)\n",
    "accuracy_knn2 = np.mean(y_test == preds_knn)*100\n",
    "\n",
    "print('The accuracy for the refined K Nearest Neighbors model is {}%'.format(accuracy_knn2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds_knn, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv = KNeighborsClassifier()\n",
    "\n",
    "cv_scores = cross_val_score(knn_cv, X, y, cv=5)\n",
    "\n",
    "print('Cross-Validation Scores: ' + str(cv_scores))\n",
    "print('Mean of Cross-Validation Scores: {}%'.format(np.mean(cv_scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "params = {'n_neighbors': [10, 5, 3, 1], 'p':[3, 2, 1]}\n",
    "\n",
    "knn_gscv = GridSearchCV(knn, params, cv=5)\n",
    "\n",
    "knn_gscv.fit(X_train, y_train)\n",
    "preds_knn = knn_gscv.predict(X_test)\n",
    "accuracy_knn_gscv = np.mean(y_test == preds_knn)*100\n",
    "\n",
    "print('The accuracy for the refined K Nearest Neighbors model is {}%'.format(accuracy_knn_gscv))\n",
    "\n",
    "# NOTE: the following commented-out code is to print\n",
    "# what the best parameter values are and \n",
    "# the mean cross-validated score of the best estimator\n",
    "print('The best parameters are:')\n",
    "for key, val in knn_gscv.best_params_.items():\n",
    "    print(str(key) + ':', val)\n",
    "# print('\\nThe best score for the refined K Nearest Neighbors model is {}%'.format(knn_gscv.best_score_*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting ROC and P-R Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "knn_best = KNeighborsClassifier(n_neighbors=3, p=3)\n",
    "knn_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass P-R curve\n",
    "visualize_data.plot_precision_recall(\n",
    "    'K Nearest Neighbors', knn_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "knn_best = KNeighborsClassifier(n_neighbors=3, p=3)\n",
    "knn_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass ROC curve\n",
    "visualize_data.plot_multiclass_roc(\n",
    "    'K Nearest Neighbors', knn_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "accuracy_svc = svc.score(X_test, y_test)*100\n",
    "print('The accuracy for the Support Vector Machine model is {}%'.format(accuracy_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_parameters = {'tol': [.1, .001, .0001], 'C':[10, 1, .1]}\n",
    "svc = SVC()\n",
    "clf3 = GridSearchCV(svc, svc_parameters)\n",
    "\n",
    "clf3.fit(X_train, y_train)\n",
    "preds_svc = clf3.predict(X_test)\n",
    "accuracy_svc2 = np.mean(y_test == preds_svc)*100\n",
    "\n",
    "print('The accuracy for the refined Support Vector Machine model is {}%'.format(accuracy_svc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds_svc, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_cv = SVC()\n",
    "\n",
    "cv_scores = cross_val_score(svc_cv, X, y, cv=5)\n",
    "\n",
    "print('Cross-Validation Scores: ' + str(cv_scores))\n",
    "print('Mean of Cross-Validation Scores: {}%'.format(np.mean(cv_scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "params = {'tol': [.1, .001, .0001], 'C':[10, 1, .1]}\n",
    "svc_gscv = GridSearchCV(svc, params, cv=5)\n",
    "\n",
    "svc_gscv.fit(X_train, y_train)\n",
    "preds_svc = svc_gscv.predict(X_test)\n",
    "accuracy_svc_gscv = np.mean(y_test == preds_svc)*100\n",
    "\n",
    "print('The accuracy for the refined Support Vector Machine model is {}%'.format(accuracy_svc_gscv))\n",
    "\n",
    "# NOTE: the following commented-out code is to print\n",
    "# what the best parameter values are and \n",
    "# the mean cross-validated score of the best estimator\n",
    "print('The best parameters are:')\n",
    "for key, val in svc_gscv.best_params_.items():\n",
    "    print(str(key) + ':', val)\n",
    "# print('\\nThe best score for the refined Support Vector Machine model is {}%'.format(svc_gscv.best_score_*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting ROC and P-R Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "svc_best = SVC(C=10, tol=0.1)\n",
    "svc_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass P-R curve\n",
    "visualize_data.plot_precision_recall(\n",
    "    'Support Vector Machine', svc_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "svc_best = SVC(C=10, tol=0.1)\n",
    "svc_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass ROC curve\n",
    "visualize_data.plot_multiclass_roc(\n",
    "    'Support Vector Machine', svc_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "accuracy_gnb = gnb.score(X_test, y_test)*100\n",
    "print('The accuracy for the Naive Bayes is {}%'.format(accuracy_gnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_parameters = {'var_smoothing':[1e-3, 1e-6, 1e-9]}\n",
    "gnb = GaussianNB()\n",
    "clf4 = GridSearchCV(gnb, gnb_parameters)\n",
    "\n",
    "clf4.fit(X_train, y_train)\n",
    "preds_gnb = clf4.predict(X_test)\n",
    "accuracy_gnb2 = np.mean(y_test == preds_gnb)*100\n",
    "\n",
    "print('The accuracy for the refined Naive Bayes model is {}%'.format(accuracy_gnb2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds_gnb, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform k-fold cross-validation\n",
    "gnb_cv = GaussianNB()\n",
    "\n",
    "%time cv_scores = cross_val_score(gnb_cv, X, y, cv=5)\n",
    "\n",
    "print('Cross-Validation Scores: ' + str(cv_scores))\n",
    "print('Mean of Cross-Validation Scores: {}%'.format(np.mean(cv_scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform hyperparameter tuning and output the best params and score\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# the 'priors' values come from the following:\n",
    "# - the first list of values is just indicating \n",
    "# equal weights between the 3 classes, which are 0, 1, and 2\n",
    "# - the second list of value refers to the weights\n",
    "# of the classes that we provided initially\n",
    "params = {#'priors': [[0.333, 0.333, 0.334], [0.55, 0.30, 0.15]],\n",
    "          'var_smoothing': [0,1e-3,1e-6, 1e-9,0.01,0.1,0.5,1]}\n",
    "\n",
    "gnb_gscv = GridSearchCV(gnb, params, cv=5)\n",
    "\n",
    "gnb_gscv.fit(X_train, y_train)\n",
    "\n",
    "preds_gnb = gnb_gscv.predict(X_test)\n",
    "accuracy_gnb_gscv = np.mean(y_test == preds_gnb)*100\n",
    "\n",
    "print('The accuracy for the refined Naive Bayes model is {}%'.format(accuracy_gnb_gscv))\n",
    "\n",
    "# NOTE: the following commented-out code is to print\n",
    "# what the best parameter values are and \n",
    "# the mean cross-validated score of the best estimator\n",
    "print('The best parameters are:')\n",
    "for key, val in gnb_gscv.best_params_.items():\n",
    "    print(str(key) + ':', val)\n",
    "# print('\\nThe best score for the refined Naive Bayes model is {}%'.format(gnb_gscv.best_score_*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting ROC and P-R Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "gnb_best = GaussianNB(priors=[0.333, 0.333, 0.334], var_smoothing=0.1)\n",
    "gnb_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass P-R curve\n",
    "visualize_data.plot_precision_recall(\n",
    "    'Naive Bayes', gnb_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "gnb_best = GaussianNB(priors=[0.333, 0.333, 0.334], var_smoothing=0.1)\n",
    "gnb_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass ROC curve\n",
    "visualize_data.plot_multiclass_roc(\n",
    "    'Naive Bayes', gnb_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "accuracy_rf = rf.score(X_test, y_test)*100\n",
    "print('The accuracy for the Random Forest is {}%'.format(accuracy_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_parameters = {'n_estimators':[100, 50, 10]}\n",
    "rf = RandomForestClassifier()\n",
    "clf5 = GridSearchCV(rf, rf_parameters)\n",
    "\n",
    "clf5.fit(X_train, y_train)\n",
    "preds_rf = clf5.predict(X_test)\n",
    "accuracy_rf2 = np.mean(y_test == preds_rf)*100\n",
    "\n",
    "print('The accuracy for the refined Random Forest model is {}%'.format(accuracy_rf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds_rf, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform k-fold cross-validation\n",
    "rf_cv = RandomForestClassifier()\n",
    "\n",
    "%time cv_scores = cross_val_score(rf_cv, X, y, cv=5)\n",
    "\n",
    "print('Cross-Validation Scores: ' + str(cv_scores))\n",
    "print('Mean of Cross-Validation Scores: {}%'.format(np.mean(cv_scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform hyperparameter tuning and output the best params and score\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# the 'priors' values come from the following:\n",
    "# - the first list of values is the default, which just indicates \n",
    "# equal weights between the 3 classes, which are 0, 1, and 2\n",
    "# - the second list of value refers to the weights\n",
    "# of the classes that we provided initially\n",
    "params = {#'class_weight': [{0: 1, 1: 1, 2: 1}, {0: 0.55, 1: 0.30, 2: 0.15}],\n",
    "          'n_estimators':[200, 100, 50, 10]}\n",
    "\n",
    "rf_gscv = GridSearchCV(rf, params, cv=5)\n",
    "\n",
    "rf_gscv.fit(X_train, y_train)\n",
    "\n",
    "preds_rf = rf_gscv.predict(X_test)\n",
    "accuracy_rf_gscv = np.mean(y_test == preds_rf)*100\n",
    "\n",
    "print('The accuracy for the refined Random Forest model is {}%'.format(accuracy_rf_gscv))\n",
    "\n",
    "# NOTE: the following commented-out code is to print\n",
    "# what the best parameter values are and \n",
    "# the mean cross-validated score of the best estimator\n",
    "print('The best parameters are:')\n",
    "for key, val in rf_gscv.best_params_.items():\n",
    "    print(str(key) + ':', val)\n",
    "# print('\\nThe best score for the refined Random Forest model is {}%'.format(rf_gscv.best_score_*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting ROC and P-R Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "rf_best = RandomForestClassifier(class_weight={0: 0.55, 1: 0.3, 2: 0.15}, \n",
    "                                 n_estimators=200)\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass P-R curve\n",
    "visualize_data.plot_precision_recall(\n",
    "    'Random Forest', rf_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model with the best parameters\n",
    "rf_best = RandomForestClassifier(class_weight={0: 0.55, 1: 0.3, 2: 0.15}, \n",
    "                                 n_estimators=200)\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "# plot multiclass ROC curve\n",
    "visualize_data.plot_multiclass_roc(\n",
    "    'Random Forest', rf_best, X_test, y_test, \n",
    "    n_classes=3, figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Logistic Regression', 'K Nearest Neighbors',\n",
    "         'Support Vector Machine', 'Naive Bayes', 'Random Forest']\n",
    "model_accuracies = [accuracy_lg, accuracy_knn, \n",
    "                    accuracy_svc, accuracy_gnb, accuracy_rf]\n",
    "gscv_accuracies = [accuracy_lr_gscv, accuracy_knn_gscv,\n",
    "                  accuracy_svc_gscv, accuracy_gnb_gscv, accuracy_rf_gscv]\n",
    "model_comparison = pd.DataFrame(\n",
    "    np.column_stack([models, model_accuracies, gscv_accuracies]),\n",
    "    columns=['Model', 'Model Accuracy', 'Grid Search CV Accuracy'])\n",
    "model_comparison.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
